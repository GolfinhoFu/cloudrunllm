# LLM V3 Service

ServiÃ§o monolÃ­tico para processamento de pitches e assistente de empreendedorismo, construÃ­do com:
- **Gemini 2.0 Flash** (processamento de Ã¡udio nativo)
- **RAG com FAISS** (base de conhecimento)
- **Flask API** (endpoints REST)
- **Google Cloud Run** (deploy serverless)

## ğŸ“‹ CaracterÃ­sticas

### âœ… Implementado
- **Modo AnimaGuy**: Chat assistente com RAG e histÃ³rico de sessÃ£o
- **Modo Pitch**: AnÃ¡lise por 4 investidores simulados (Shark Tank)
- **Processamento de Ãudio**: Gemini processa Ã¡udio nativamente (sem STT separado)
- **RAG Completo**: Base de conhecimento com FAISS
- **Auto-scaling**: Cloud Run gerencia instÃ¢ncias automaticamente
- **ValidaÃ§Ã£o Robusta**: Validadores de entrada para seguranÃ§a

### ğŸ¯ ConfiguraÃ§Ã£o Final
- **Escalonamento**: Auto-scaling nativo (sem min/max instances)
- **Custo Estimado**: ~$5-15/mÃªs para <10 req/dia
- **Performance**: 
  - Cold start: 5-8s
  - Warm: 2-8s (dependendo do modo)

## ğŸ—ï¸ Arquitetura

```
llm_V3/
â”œâ”€â”€ main.py                 # Flask API principal
â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ Dockerfile            # Container definition
â”œâ”€â”€ deploy.sh             # Script de deploy
â”œâ”€â”€ .dockerignore         # Arquivos ignorados no build
â”‚
â”œâ”€â”€ handlers/             # Processadores de requisiÃ§Ãµes
â”‚   â”œâ”€â”€ animaguy_handler.py
â”‚   â””â”€â”€ pitch_handler.py
â”‚
â”œâ”€â”€ services/             # IntegraÃ§Ãµes externas
â”‚   â”œâ”€â”€ rag_service.py       # FAISS + embeddings
â”‚   â”œâ”€â”€ gemini_service.py    # Google Gemini API
â”‚   â””â”€â”€ storage_service.py   # Google Cloud Storage
â”‚
â”œâ”€â”€ models/               # Prompts e templates
â”‚   â””â”€â”€ prompts.py
â”‚
â””â”€â”€ utils/                # UtilitÃ¡rios
    â”œâ”€â”€ firestore_client.py  # HistÃ³rico de sessÃµes
    â””â”€â”€ validators.py        # ValidaÃ§Ã£o de inputs
```

## ğŸš€ Deploy

### PrÃ©-requisitos

1. **Google Cloud Project** configurado
2. **Gemini API Key** obtida
3. **GCS Bucket** com Ã­ndice RAG:
   - `faiss_index.bin`
   - `text_chunks.json`
4. **gcloud CLI** instalado e autenticado

### Passos

1. **Clone/Navegue atÃ© o diretÃ³rio**:
```bash
cd llm_V3
```

2. **Edite o `deploy.sh`** com suas configuraÃ§Ãµes:
```bash
PROJECT_ID="seu-project-id"
GCS_RAG_BUCKET_NAME="seu-rag-bucket"
GEMINI_API_KEY="sua-api-key"
```

3. **Execute o deploy**:
```bash
chmod +x deploy.sh
./deploy.sh
```

4. **Aguarde** o deploy completar (~5-10 minutos)

## ğŸ“¡ API

### Endpoint: `/process`

**MÃ©todo**: `POST`  
**Content-Type**: `multipart/form-data`

### Modo AnimaGuy (Chat)

**Request**:
```bash
curl -X POST https://seu-servico.run.app/process \
  -F 'mode=animaguy' \
  -F 'text=Como fazer um bom pitch?' \
  -F 'session_id=opcional-session-id'
```

**Response**:
```json
{
  "answer": "Resposta do AnimaGuy...",
  "session_id": "uuid-da-sessao"
}
```

### Modo Pitch (AnÃ¡lise)

**Request com Ãudio**:
```bash
curl -X POST https://seu-servico.run.app/process \
  -F 'mode=pitch' \
  -F 'audio_file=@pitch.mp3'
```

**Request com Texto**:
```bash
curl -X POST https://seu-servico.run.app/process \
  -F 'mode=pitch' \
  -F 'text=Meu pitch Ã© sobre...'
```

**Response**:
```json
{
  "investor_feedbacks": [
    {
      "investor": "O CÃ©tico",
      "persona": "Focado em nÃºmeros...",
      "investorAnswer": "AnÃ¡lise... Estou fora.",
      "score": 7.5
    },
    // ... mais 3 investidores
  ],
  "transcription_text": ""
}
```

### Health Check

```bash
curl https://seu-servico.run.app/health
```

## ğŸ”§ ConfiguraÃ§Ã£o Local

### Para desenvolvimento:

1. **Crie ambiente virtual**:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

2. **Instale dependÃªncias**:
```bash
pip install -r requirements.txt
```

3. **Configure variÃ¡veis de ambiente**:
```bash
export PROJECT_ID="seu-project-id"
export GCS_RAG_BUCKET_NAME="seu-rag-bucket"
export GEMINI_API_KEY="sua-api-key"
```

4. **Baixe Ã­ndice RAG manualmente** ou coloque em `knowledge_base/`:
   - `faiss_index.bin` â†’ `/tmp/faiss_index.bin`
   - `text_chunks.json` â†’ `/tmp/text_chunks.json`

5. **Execute localmente**:
```bash
python main.py
```

Acesse: `http://localhost:8080`

## ğŸ“¦ DependÃªncias Principais

- **Flask 3.0**: Framework web
- **gunicorn 21.2**: Servidor WSGI
- **google-generativeai 0.3**: API Gemini
- **faiss-cpu 1.7**: Busca vetorial
- **google-cloud-storage 2.10**: GCS
- **google-cloud-firestore 2.13**: Firestore

## ğŸ” Monitoramento

### Logs no Cloud Run:
```bash
gcloud run services logs read llm-v3-service \
  --region=southamerica-east1 \
  --limit=50
```

### MÃ©tricas:
- Acesse: [Cloud Console](https://console.cloud.google.com/run)
- Monitore: LatÃªncia, Erros, Uso de CPU/RAM

## âš™ï¸ ParÃ¢metros de Performance

- **Memory**: 2GB
- **CPU**: 1 vCPU
- **Timeout**: 300s (5 minutos)
- **CPU Boost**: Habilitado (reduz cold start)
- **Concurrency**: 80 (padrÃ£o Cloud Run)

## ğŸ› Troubleshooting

### Cold Start Lento
- O Ã­ndice RAG Ã© baixado na inicializaÃ§Ã£o (~2-3s)
- CPU Boost jÃ¡ estÃ¡ habilitado
- Normal para primeira requisiÃ§Ã£o apÃ³s idle

### RAG NÃ£o Funciona
- Verifique se o bucket GCS existe
- Confirme que `faiss_index.bin` e `text_chunks.json` estÃ£o no bucket
- Veja logs para erros de download

### Erro de API Key
- Valide a `GEMINI_API_KEY` nas variÃ¡veis de ambiente
- Confirme que a key tem permissÃµes para Gemini API

## ğŸ“ PrÃ³ximos Passos (Opcional)

- [ ] Adicionar Google Search Grounding para AnimaGuy
- [ ] Implementar cache de embeddings
- [ ] Adicionar mÃ©tricas customizadas
- [ ] CI/CD pipeline automatizado
- [ ] Testes unitÃ¡rios e de integraÃ§Ã£o

## ğŸ“„ LicenÃ§a

Projeto interno - Anima EducaÃ§Ã£o

---

**VersÃ£o**: 3.0  
**Ãšltima atualizaÃ§Ã£o**: Outubro 2025  
**Autor**: Alexandre
